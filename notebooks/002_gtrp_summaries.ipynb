{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('./.env')\n",
    "_SUMMARY_MODEL = \"llama3.2:3b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Learning with llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chromadb store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader, JSONLoader\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def metadata_func(record:dict, metadata:dict) -> dict:\n",
    "    for col_metadata in ['code','title','code_data','code_number','link']:\n",
    "        metadata[col_metadata] = record.get(col_metadata)\n",
    "    metadata['authors'] = ','.join(record.get('authors'))\n",
    "    return metadata\n",
    "\n",
    "def load_chunk_persist_pdf(pdf_folder_path = \"E:\\codes\\Artigo Forecast\\pdfs\",\n",
    "                           collection_name = 'article_collection',\n",
    "                           json_metadata_function = metadata_func,\n",
    "                           chunk_size=2000, chunk_overlap=100) -> Chroma:\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        file_path = os.path.join(pdf_folder_path, file)\n",
    "        if file.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "        # if file.endswith('.json'):\n",
    "        #     data = json.loads(Path(file_path).read_text())\n",
    "        #     loader = JSONLoader(\n",
    "        #         file_path=file_path,\n",
    "        #         jq_schema='.summary[]',\n",
    "        #         content_key='summary',\n",
    "        #         metadata_func=metadata_func,\n",
    "        #         text_content=False)\n",
    "        #     documents.extend(loader.load())\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    client = chromadb.Client()\n",
    "    try:\n",
    "        client.delete_collection(collection_name)\n",
    "        consent_collection = client.create_collection(collection_name)\n",
    "    except:\n",
    "        consent_collection = client.create_collection(collection_name)\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "        persist_directory=\"../.chroma_store\"\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "def _result_print(result):\n",
    "    print(\"-=\"*40)\n",
    "    print(f\"-- input: \\n{result['input']}\")\n",
    "    print(\"-=\"*40)\n",
    "    print(f\"-- context: \\n\")\n",
    "    for docs in result['context']:\n",
    "        print(\"--\"*40)\n",
    "        print(\"-- article:\")\n",
    "        print(docs.metadata['source'])\n",
    "        print('-- content:')\n",
    "        print(docs.page_content)\n",
    "    print(\"-=\"*40)\n",
    "    print(f\"-- answer: \\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_articles = [\n",
    "    'E:\\\\codes\\\\Artigo Forecast\\\\pdfs\\\\2309.13807v1.pdf',\n",
    "    'E:\\\\codes\\\\Artigo Forecast\\\\pdfs\\\\2309.14518v3.pdf',\n",
    "    'E:\\\\codes\\\\Artigo Forecast\\\\pdfs\\\\2310.11059v1.pdf',\n",
    "    'E:\\\\codes\\\\Artigo Forecast\\\\pdfs\\\\2405.19729v1.pdf',\n",
    "    'E:\\\\codes\\\\Artigo Forecast\\\\pdfs\\\\2406.04390v1.pdf',\n",
    "    'E:\\\\codes\\\\Artigo Forecast\\\\pdfs\\\\2409.04542v1.pdf',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tools\n",
    "We will also define some tools that our agents will use in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_articles = {}\n",
    "for article in _articles:\n",
    "    loader = PyPDFLoader(article)\n",
    "    list_summaries = []\n",
    "    for page in loader.load():\n",
    "        llm = ChatOpenAI(\n",
    "                    api_key=\"ollama\",\n",
    "                    model=\"llama3.2:3b\",\n",
    "                    base_url=\"http://localhost:11434/v1\",\n",
    "                )\n",
    "        model = ChatOpenAI(\n",
    "            api_key=\"ollama\",\n",
    "            model=_SUMMARY_MODEL,\n",
    "            base_url=\"http://localhost:11434/v1\",\n",
    "        )\n",
    "        message = HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": f\"\"\"\n",
    "                    resume the content of a text. you have to point the feature selection models used, the context where they are using it, the relationship with time series, and the results of the text, summarized.\n",
    "                    never return authors or references, just the main concepts mentioned here.\n",
    "                    this is the text:\n",
    "                    <<{page.page_content}>>\n",
    "                    \"\"\"}\n",
    "            ],\n",
    "        )\n",
    "        list_summaries.append(model.invoke([message]).content)\n",
    "        message = HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": f\"\"\"\n",
    "                    resume the content of a text. you have to point the feature selection models used, the context where they are using it, the relationship with time series, and the results of the text, summarized.\n",
    "                    never return authors or references, just the main concepts mentioned here.\n",
    "                    this is the text:\n",
    "                    <<{'|'.join(list_summaries)}>>\n",
    "                    \"\"\"}\n",
    "            ],\n",
    "        )\n",
    "        dict_articles[article] = {\n",
    "            'list_summaries':list_summaries, \n",
    "            'final_summary':model.invoke([message]).content\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "\n",
    "# Make it work for Python 2+3 and with Unicode\n",
    "import io\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str\n",
    "\n",
    "# Define data\n",
    "data = dict_articles\n",
    "\n",
    "# Write JSON file\n",
    "with io.open('summaries.json', 'w', encoding='utf8') as outfile:\n",
    "    str_ = json.dumps(data,\n",
    "                      indent=4, sort_keys=True,\n",
    "                      separators=(',', ': '), ensure_ascii=False)\n",
    "    outfile.write(to_unicode(str_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, io\n",
    "with io.open('summaries.json', 'r', encoding='utf8') as outfile:\n",
    "    dict_articles = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "E:\\codes\\Artigo Forecast\\pdfs\\2309.13807v1.pdf\n",
      "Here is the revised version of the summary:\n",
      "\n",
      "**Feature Selection Models:**\n",
      "\n",
      "1. FFORMA (Feature-based Forecast Model Averaging)\n",
      "2. feasts (Feature Extraction and Statistics for Time Series)\n",
      "3. ReliefF and RReliefF (Machine learning algorithms for feature selection)\n",
      "4. Meta-learning approaches (using machine learning to select time series models)\n",
      "\n",
      "**Context:**\n",
      "\n",
      "1. Used in time series forecasting, demand forecasting, and other applications where selecting the best model is crucial.\n",
      "2. Context of competitions such as M4 and M5, where multiple methods are evaluated on a large dataset.\n",
      "\n",
      "**Relationship with Time Series:**\n",
      "\n",
      "1. Many models are specifically designed for time series data, taking into account temporal dependencies and patterns.\n",
      "2. Some models use techniques like bootstrapping and maximum entropy to handle time series data.\n",
      "3. Probabilistic time series forecasting is an area that requires further study, considering feature uncertainty.\n",
      "\n",
      "**Results:**\n",
      "\n",
      "1. Improved forecast accuracy and robustness using feature-based methods.\n",
      "2. Combining multiple models can lead to better performance, but requires careful selection of features and methods.\n",
      "3. Meta-learning approaches show promise in selecting the best model for a specific time series problem.\n",
      "4. The importance of feature extraction and statistics is highlighted, with feasts being a useful tool for this purpose.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "1. Feature-based methods can improve forecast accuracy.\n",
      "2. Combining multiple models can lead to better performance.\n",
      "3. Meta-learning approaches are effective in selecting the best model.\n",
      "4. Feature extraction and statistics play a crucial role in time series forecasting.\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "E:\\codes\\Artigo Forecast\\pdfs\\2309.14518v3.pdf\n",
      "It seems like you've summarized a table from an experiment involving machine learning models, specifically focusing on pruning techniques for different datasets (ROCKET, MiniRocket, MultiRocket). The table provides insights into how pruning affects the test accuracy of these models.\n",
      "\n",
      "Here are some key takeaways:\n",
      "\n",
      "1. **Pruning impact**: Pruning has a moderate to significant impact on the accuracy of the full model, with most datasets showing an improvement in accuracy after pruning.\n",
      "2. **Dataset-specific variations**: The effect of pruning on accuracy varies across different datasets (ROCKET, MiniRocket, MultiRocket). Some datasets show more significant improvements or minimal changes, while others have less pronounced effects.\n",
      "3. **Improvement trends**: The improvement trend is consistent across most datasets, with most values ranging from +0.27 to +0.37 in the \"Relative Accuracy Change\" column.\n",
      "\n",
      "These findings suggest that pruning can be an effective technique for improving model accuracy on various datasets. However, the effect of pruning may depend on the specific characteristics of each dataset.\n",
      "\n",
      "To further understand the implications of these results, you might want to:\n",
      "\n",
      "1. **Analyze the datasets**: Study the properties and characteristics of each dataset (e.g., size, distribution, features) to better understand why pruning has varying effects.\n",
      "2. **Compare pruning techniques**: Investigate different pruning methods and their relative performances on various datasets.\n",
      "3. **Explore other machine learning techniques**: Examine how other techniques, such as feature selection or model ensembling, compare to pruning in terms of accuracy improvement.\n",
      "\n",
      "If you have any specific questions about the interpretation of these results or would like further suggestions on how to proceed with this analysis, feel free to ask!\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "E:\\codes\\Artigo Forecast\\pdfs\\2310.11059v1.pdf\n",
      "Here's a summary of the content in a concise format:\n",
      "\n",
      "**Feature Selection Models:**\n",
      "\n",
      "1. Autoregression\n",
      "2. Forward CMI (Conditional Mutual Information)\n",
      "3. Backward CMI (Conditional Mutual Information)\n",
      "4. FullCI linear and nonLinear (Full Conditional Independence)\n",
      "5. PCMCI linear and nonLinear (Partial Conditional Mutual Information)\n",
      "6. TEFS (Time-Evolving Feature Selection) (custom implementation)\n",
      "\n",
      "**Context:**\n",
      "\n",
      "Real-world experiments on climate datasets with 5 features each, and one dataset with 15 features.\n",
      "\n",
      "**Relationship with Time Series:**\n",
      "\n",
      "Most models handle time series data, considering temporal dependencies between variables.\n",
      "\n",
      "**Results:**\n",
      "\n",
      "Variable performance of each model varies across time-depths. TEFS shows similar performance to other methods for some variables, while others perform poorly. FullCI linear and nonLinear tend to perform well, especially when considering multiple variables at once.\n",
      "\n",
      "Let me know if you'd like me to rephrase or expand on this summary!\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "E:\\codes\\Artigo Forecast\\pdfs\\2405.19729v1.pdf\n",
      "Here is a summary of the text:\n",
      "\n",
      "**Feature Selection Methods:**\n",
      "\n",
      "1. Relief-based feature selection method\n",
      "2. Cost-sensitive analysis approach for multiple time series prediction\n",
      "3. Other notable mentions include LSTM networks, CatBoost algorithm, and deep reinforcement learning methods.\n",
      "\n",
      "**Context:**\n",
      "\n",
      "* The text describes using feature selection methods in various domains, including medical data analysis.\n",
      "* It highlights the importance of selecting relevant features to improve prediction accuracy.\n",
      "\n",
      "**Relationship with Time Series:**\n",
      "\n",
      "* Not explicitly stated, but likely related to time series analysis due to the mention of \"time series prediction\" and \"features with high frequencies\".\n",
      "\n",
      "**Results:**\n",
      "\n",
      "* Proposed methods can improve prediction accuracy in various domains while considering feature acquisition and use costs.\n",
      "* Relief-based feature selection is effective in identifying informative features for time series prediction.\n",
      "* Cost-sensitive learning approaches can lead to more accurate and efficient decision-making processes.\n",
      "\n",
      "**Other notable mentions:**\n",
      "\n",
      "1. LSTM networks are widely used in neural computing for natural language processing, speech recognition, and time series forecasting.\n",
      "2. CatBoost algorithm is an unbiased boosting algorithm suitable for categorical features.\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "E:\\codes\\Artigo Forecast\\pdfs\\2406.04390v1.pdf\n",
      "It seems like you didn't provide a text for me to summarize, but instead, you provided an outline of various summaries of different texts.\n",
      "\n",
      "Please provide the actual text you'd like me to summarize, and I'll be happy to help!\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "E:\\codes\\Artigo Forecast\\pdfs\\2409.04542v1.pdf\n",
      "Based on the provided text, I can extract some key points about feature selection models in the context of time series forecasting:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Feature selection**: The study introduces a novel approach to feature selection that mitigates the influence of outlier features and achieves comparable results to an initially proposed model with a smaller set of parameters and features.\n",
      "2. **Interval-based features**: Time series forest classifiers use interval-based features generated from sliding window operations.\n",
      "3. **Transforming pooling features**: Multivariate time series classification models utilize transforming pooling features.\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "1. **Bag-of-features framework**: Used for classifying time series data, with a focus on mitigating extreme class-imbalance.\n",
      "2. **Time series forest**: Another approach used for time series classification and feature extraction.\n",
      "3. **Multivariate time series dataset**: A dataset specifically designed for space weather data analytics.\n",
      "\n",
      "**Results:**\n",
      "\n",
      "1. Improved performance in solar flare prediction using machine learning algorithms and time series features.\n",
      "2. Comparable results to initially proposed models with a smaller set of parameters and features.\n",
      "3. Streamlined models achieve similar accuracy as complex counterparts that utilize all 24 features.\n",
      "\n",
      "**Relationship with Time Series:**\n",
      "\n",
      "1. **Time series classification**: The models are specifically designed for handling time series data, which is relevant to predicting solar flares and analyzing space weather phenomena.\n",
      "2. **Space weather data analytics**: The multivariate time series dataset is used for space weather data analytics.\n",
      "\n",
      "Please let me know if you'd like me to clarify or expand on any of these points!\n"
     ]
    }
   ],
   "source": [
    "for _key in dict_articles:\n",
    "    print(\"-=\"*20)\n",
    "    print(_key)\n",
    "    print(dict_articles[_key]['final_summary'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
